
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta property="og:title" content="Transformation properties of the likelihood and posterior" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://cranmer.github.io/stats-ds-book/distributions/invariance-of-likelihood-to-reparameterizaton.html" />
  <meta property="og:description" content="In the notebooks How do distributions transform under a change of variables? and Change of variables with autodiff we saw how a distribution p_X for a random variable X transforms under a change of..." />
  <meta property="og:image" content="https://cranmer.github.io/stats-ds-book/_images/Neyman-pearson.006.png" />
  <meta property="og:image:alt" content="Transformation properties of the likelihood and posterior" />
  
    <title>Transformation properties of the likelihood and posterior &#8212; Statistics and Data Science</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx_tabs/semantic-ui-2.4.1/segment.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx_tabs/semantic-ui-2.4.1/menu.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx_tabs/semantic-ui-2.4.1/tab.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx_tabs/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/pdf_print.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/save_state.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": "\\mathbb{N}", "indep": "{\\perp\\kern-5pt\\perp}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "bered": ["\\color{#DC2830}{#1}", 1], "ecol": ["}}"]}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Investigating propagation of errors" href="../error-propagation/investigating-propagation-of-errors.html" />
    <link rel="prev" title="Transformation of likelihood with change of random variable" href="likelihood-change-obs.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Statistics and Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Statistics and Data Science
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About the course
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../schedule.html">
   Draft Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../jupyterhub.html">
   JupyterHub for class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nbgrader.html">
   nbgrader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../discussion_forum.html">
   Discussion Forum
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminaries.html">
   Preliminaries
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Probability
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../probability-topics.html">
   Probability Topics
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../random_variables.html">
     Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../conditional.html">
     Conditonal Probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayes_theorem.html">
     Bayes’ Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../independence.html">
     Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../empirical_distribution.html">
     Empirical Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../expectation.html">
     Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../correlation.html">
     Covariance and Correlation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../datasaurus-long.html">
     Simple data exploration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="visualize_marginals.html">
     Visualizing joint and marginal distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../measures_of_dependence.html">
     Quantifying statistical dependence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="change-of-variables.html">
     How do distributions transform under a change of variables ?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="one-over-x-flow.html">
     Change of variables with autodiff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="likelihood-change-obs.html">
     Transformation of likelihood with change of random variable
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Transformation properties of the likelihood and posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../error-propagation/investigating-propagation-of-errors.html">
     Investigating propagation of errors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../error-propagation/error_propagation_with_jax.html">
     Revisiting error propagation with automatic differentiation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="accept-reject.html">
     Accept / Reject Monte Carlo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Binomial_histograms-interactive.html">
     An interactive exploration of statistical fluctuations in histograms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../pgm/daft.html">
     Visualizing Graphical Models
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../statistics-topics.html">
   Statistics Topics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/estimators.html">
     Estimators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/bias-variance.html">
     Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/investigation-bessels-correction.html">
     Investigation of Bessel’s correction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/cramer-rao-bound.html">
     Cramér-Rao Bound
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/consistency.html">
     Consistency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/sufficiency.html">
     Sufficiency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/information-geometry.html">
     Information Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/neyman_pearson.html">
     Neyman-Pearson lemma
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/neyman_construction.html">
     Neyman construction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/lhc_stats_thumbnail.html">
     Thumbnail of LHC Statistical Procedures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistics/statistical_decision_theory.html">
     Statistical decision theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../probprog/MarkovPath.html">
     Universal Probabilistic Programming Example
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../prml_notebooks/attribution.html">
   PRML Examples
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch01_Introduction.html">
     1. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch02_Probability_Distributions.html">
     2. Probability Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch03_Linear_Models_for_Regression.html">
     3. Linear Models for Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch04_Linear_Models_for_Classfication.html">
     4. Linear Models for Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch05_Neural_Networks.html">
     5. Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch06_Kernel_Methods.html">
     6. Kernel Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch07_Sparse_Kernel_Machines.html">
     7. Sparse Kernel Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch08_Graphical_Models.html">
     8. Graphical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch09_Mixture_Models_and_EM.html">
     9. Mixture Models and EM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch10_Approximate_Inference.html">
     10. Approximate Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch11_Sampling_Methods.html">
     11. Sampling Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch12_Continuous_Latent_Variables.html">
     12. Continuous Latent Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../prml_notebooks/ch13_Sequential_Data.html">
     13. Sequential Data
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Software and Computing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../computing-topics.html">
   Software &amp; Computing Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../autodiff-tutorial.html">
   Tutorial on Automatic Differentiation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data Science
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../data-science-topics.html">
   Data Science, what is it?
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../other_resources.html">
   Other Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../built-on.html">
   Built on
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Jupyter Book Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cheatsheet.html">
   MyST Cheat Sheet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Content with notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../interactive.html">
   Interactive data visualizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../test_embed_video.html">
   Test Embed Video
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../color-in-equations.html">
   Color in equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../test-sphinxext-opengraph.html">
   Test Sphinxext-opengraph
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/distributions/invariance-of-likelihood-to-reparameterizaton.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/cranmer/stats-ds-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/cranmer/stats-ds-book/issues/new?title=Issue%20on%20page%20%2Fdistributions/invariance-of-likelihood-to-reparameterizaton.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/cranmer/stats-ds-book/edit/master/book/distributions/invariance-of-likelihood-to-reparameterizaton.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/cranmer/stats-ds-book/master?urlpath=tree/book/distributions/invariance-of-likelihood-to-reparameterizaton.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/cranmer/stats-ds-book/blob/master/book/distributions/invariance-of-likelihood-to-reparameterizaton.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-does-the-likelihood-transform-to-reparameterization">
   How does the likelihood transform to reparameterization?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#check-explicitly">
     Check explicitly
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluate-the-likelihoods">
     Evaluate the likelihoods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#make-matched-pairs-for-the-parameters">
     Make matched pairs for the parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-the-likelihoods">
     Plot the likelihoods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#equivariance-of-the-mle">
   Equivariance of the MLE
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Check explicitly
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparison-to-maximum-a-posteriori-map-estimate">
   Comparison to maximum a posteriori (MAP) estimate
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-myth-of-the-uninformative-uniform-prior">
   The myth of the uninformative uniform prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demonstration-of-likelihood-vs-posterior-under-reparameterization">
   Demonstration of likelihood vs. posterior under reparameterization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-the-posterior">
     Sampling the posterior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalizing-the-posterior">
     Normalizing the posterior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#make-the-plot-for-sigma">
     Make the plot for
     <span class="math notranslate nohighlight">
      \(\sigma\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#make-the-same-plot-for-v">
     Make the same plot for
     <span class="math notranslate nohighlight">
      \(V\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#violation-of-equivaraince-for-the-map-and-posterior">
     Violation of equivaraince for the MAP and posterior
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#food-for-thought">
   Food for thought
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="transformation-properties-of-the-likelihood-and-posterior">
<h1>Transformation properties of the likelihood and posterior<a class="headerlink" href="#transformation-properties-of-the-likelihood-and-posterior" title="Permalink to this headline">¶</a></h1>
<p>In the notebooks <a class="reference internal" href="change-of-variables.html"><span class="doc std std-doc"><em>How do distributions transform under a change of variables?</em>
</span></a> and <em><a class="reference internal" href="one-over-x-flow.html"><span class="doc std std-doc">Change of variables with autodiff</span></a></em> we saw how a distribution <span class="math notranslate nohighlight">\(p_X\)</span> for a random variable <span class="math notranslate nohighlight">\(X\)</span> transforms under a change of variables <span class="math notranslate nohighlight">\(X \to Y\)</span>. In particular we found that that the transformation involves a Jacobian factor.</p>
<div class="amsmath math notranslate nohighlight" id="equation-53373b1f-55c0-478d-8439-d4738f9a1d63">
<span class="eqno">(18)<a class="headerlink" href="#equation-53373b1f-55c0-478d-8439-d4738f9a1d63" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p_Y(Y=y) = p_X(x) \,/ \,\left| \, {dy}/{dx}\, \right | 
\end{equation}\]</div>
<p>If we were working with a family of distributions <span class="math notranslate nohighlight">\(p_X(X | \theta)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter that is used to index the family of distributions we would have the same thing.</p>
<div class="amsmath math notranslate nohighlight" id="equation-3048fafd-c133-40aa-a60b-910debbd28f8">
<span class="eqno">(19)<a class="headerlink" href="#equation-3048fafd-c133-40aa-a60b-910debbd28f8" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p_Y(Y=y \mid \theta) = p_X(x \mid \theta) \,/ \,\left| \, {dy}/{dx}\, \right | 
\end{equation}\]</div>
<p>In <a class="reference internal" href="likelihood-change-obs.html"><span class="doc std std-doc"><em>Transformation of likelihood with change of random variable</em></span></a> we saw how this Jacobian factor leads to a multipliciative scaling of the likelihood function (or a constant shift of the log-likelihood function), but that likelihood-ratios are invariant to a change of variables <span class="math notranslate nohighlight">\(X \to Y\)</span> (because the Jacobian factor cancels).</p>
<div class="section" id="how-does-the-likelihood-transform-to-reparameterization">
<h2>How does the likelihood transform to reparameterization?<a class="headerlink" href="#how-does-the-likelihood-transform-to-reparameterization" title="Permalink to this headline">¶</a></h2>
<p>But how does the likelihood <span class="math notranslate nohighlight">\(L(\theta) := p(X=x | \theta)\)</span> transform when we keep the random variable unchanged, but instead reparameterize the parameters used to index the family of distributions, eg. <span class="math notranslate nohighlight">\(\theta \to \phi\)</span>? As we will see, <strong>the likelihood is invariant to reparameterization</strong>, which is a very important property. It also underscores how the likelihood is not a probability density in <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This notebook is intended to demonstrate explicitly the transformation properties of the likelihood function, posterior, maximum likelihood estimate(MLE), and maximum a posterior (MAP) estimate.</p>
</div>
<p>We will start with a family of pdfs <span class="math notranslate nohighlight">\(p(X \mid \sigma)\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> is a continuous random variable and <span class="math notranslate nohighlight">\(\sigma\)</span> is the parameter used to index or parametrize the family.  In practice, we will use a normal distribution with mean 0 and <span class="math notranslate nohighlight">\(\sigma\)</span> will be the standard deviation.</p>
<p>We will then consider a reparametrization of the same family of pdfs, but instead of using <span class="math notranslate nohighlight">\(\sigma\)</span> we will use <span class="math notranslate nohighlight">\(V=\sigma^2\)</span> (eg. the variance).</p>
<p>We will consider a small data set <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^{15} \sim p(X \mid \sigma=\sigma_{true})\)</span>.</p>
<p>We will plot the distribution for two values of <span class="math notranslate nohighlight">\(\sigma\)</span>: <span class="math notranslate nohighlight">\(\sigma_{true}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{other}\)</span>.</p>
<p>We will then look at the likelihood for the individual data points <span class="math notranslate nohighlight">\(p(x_i | \sigma)\)</span> and since the data are i.i.d. the total likelihood is <span class="math notranslate nohighlight">\(L(\sigma) = \prod_i p(x_i | \sigma)\)</span>.</p>
<p>We will then look at the reparametrized likelihood for the individual data points <span class="math notranslate nohighlight">\(p(x_i | V)\)</span> and since the data are i.i.d. the total likelihood is <span class="math notranslate nohighlight">\(L^\prime(V) = \prod_i p^\prime(x_i | V)\)</span>.</p>
<p>We will then plot the original and reparametrized likelihoods in a particular way to demonstrate that the value of the likelihood is invariant to reprametrization: eg.  <span class="math notranslate nohighlight">\(L(\sigma) = L^\prime(V(\sigma))\)</span></p>
<div class="section" id="check-explicitly">
<h3>Check explicitly<a class="headerlink" href="#check-explicitly" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">rcParams</span>
<span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">})</span>
<span class="c1">#rcParams.update({&#39;font.size&#39;: 18,&#39;text.usetex&#39;: True})</span>
<span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span><span class="mi">6</span>
</pre></div>
</div>
</div>
</div>
<p>We start with a family of pdfs <span class="math notranslate nohighlight">\(p(X \mid \sigma)\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> is a continuous random variable and <span class="math notranslate nohighlight">\(\sigma\)</span> is the parameter used to index or parametrize the family.  In practice, we will use a normal distribution with mean 0 and <span class="math notranslate nohighlight">\(\sigma\)</span> will be the standard deviation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_family_of_pdfs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="c1">#return 1./np.sqrt(2.*np.pi)/sigma * np.exp( -(x-mu)**2/2/sigma**2 )</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will then consider a reparametrization of the same family of pdfs, but instead of using <span class="math notranslate nohighlight">\(\sigma\)</span> we will use <span class="math notranslate nohighlight">\(V=\sigma^2\)</span> (eg. the variance).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigma_to_V</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigma</span><span class="o">*</span><span class="n">sigma</span>

<span class="k">def</span> <span class="nf">V_to_sigma</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_reparametrized_family_of_pdfs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">my_family_of_pdfs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">V_to_sigma</span><span class="p">(</span><span class="n">variance</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We will consider a small data set <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^{15} \sim p(X \mid \sigma=\sigma_{true})\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_sigma</span><span class="o">=</span><span class="mf">1.3</span>
<span class="n">true_V</span> <span class="o">=</span> <span class="n">sigma_to_V</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate some synthetic data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0.93103086,  0.25461656,  0.01924056,  0.12333217,  0.88722697,
       -1.17641472,  0.44889527, -1.6312956 ,  0.11686814,  0.12861621,
        1.63503248, -0.2726336 ,  1.43866873, -1.53739523, -2.56144692])
</pre></div>
</div>
</div>
</div>
<p>We will plot the distribution for two values of <span class="math notranslate nohighlight">\(\sigma\)</span>: <span class="math notranslate nohighlight">\(\sigma_{true}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{other}\)</span>.</p>
<p>We will then look at the likelihood for the individual data points <span class="math notranslate nohighlight">\(p(x_i | \sigma)\)</span> and since the data are i.i.d. the total likelihood is <span class="math notranslate nohighlight">\(L(\sigma) = \prod_i p(x_i | \sigma)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">likelihood_i</span> <span class="o">=</span> <span class="n">my_family_of_pdfs</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">)</span>
<span class="n">total_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">likelihood_i</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the total likeihood for this data set when sigma=</span><span class="si">{0}</span><span class="s1"> is </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">,</span> <span class="n">total_likelihood</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-2 log L(sigma=</span><span class="si">{0}</span><span class="s1">) = </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">total_likelihood</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>the total likeihood for this data set when sigma=1.3 is 5.840111269517725e-11
-2 log L(sigma=1.3) = 47.12737234655253
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">other_sigma</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">other_V</span> <span class="o">=</span> <span class="n">sigma_to_V</span><span class="p">(</span><span class="n">other_sigma</span><span class="p">)</span>

<span class="n">other_likelihood_i</span> <span class="o">=</span> <span class="n">my_family_of_pdfs</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">other_sigma</span><span class="p">)</span>
<span class="n">other_total_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">other_likelihood_i</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the total likeihood for this data set when sigma=</span><span class="si">{0}</span><span class="s1"> is </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">other_sigma</span><span class="p">,</span> <span class="n">other_total_likelihood</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-2 log L(sigma=</span><span class="si">{0}</span><span class="s1">) = </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">other_sigma</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">other_total_likelihood</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>the total likeihood for this data set when sigma=0.7 is 3.8317371491953943e-13
-2 log L(sigma=0.7) = 57.1805758896008
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This isn&#39;t data, just a scan over x-values for plotting</span>
<span class="n">x_for_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_density</span> <span class="o">=</span> <span class="n">my_family_of_pdfs</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span><span class="n">true_sigma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span> <span class="n">true_density</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$p(x \mid \sigma=\sigma_</span><span class="si">{true}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">likelihood_i</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$L_i(\sigma) = p(x_i \mid \sigma=\sigma_</span><span class="si">{true}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">other_density</span> <span class="o">=</span> <span class="n">my_family_of_pdfs</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span><span class="n">other_sigma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span> <span class="n">other_density</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$p(x \mid \sigma=\sigma_</span><span class="si">{other}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">other_likelihood_i</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$L_i(\sigma) = p(x_i \mid \sigma=\sigma_</span><span class="si">{other}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x \mid \sigma=\sigma_</span><span class="si">{true}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_14_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_14_0.png" />
</div>
</div>
<p>We will then look at the reparametrized likelihood for the individual data points <span class="math notranslate nohighlight">\(p(x_i | V)\)</span> and since the data are i.i.d. the total likelihood is <span class="math notranslate nohighlight">\(L^\prime(V) = \prod_i p^\prime(x_i | V)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_density</span> <span class="o">=</span> <span class="n">my_reparametrized_family_of_pdfs</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span><span class="n">true_V</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span> <span class="n">true_density</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$p^\prime(x \mid V=V_</span><span class="si">{true}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">likelihood_i</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$L_i(\sigma) = p^\prime(x_i \mid V=V_</span><span class="si">{true}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">other_density</span> <span class="o">=</span> <span class="n">my_reparametrized_family_of_pdfs</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span><span class="n">other_V</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_for_plot</span><span class="p">,</span> <span class="n">other_density</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$p^\prime(x \mid V=V_</span><span class="si">{other}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">other_likelihood_i</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$L_i^\prime(\sigma) = p^\prime(x_i \mid V=V_</span><span class="si">{other}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x \mid \sigma=\sigma_</span><span class="si">{true}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_16_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_16_0.png" />
</div>
</div>
</div>
<div class="section" id="evaluate-the-likelihoods">
<h3>Evaluate the likelihoods<a class="headerlink" href="#evaluate-the-likelihoods" title="Permalink to this headline">¶</a></h3>
<p>Since the data are i.i.d. the total likelihood is <span class="math notranslate nohighlight">\(L(\sigma) = \prod_i p(x_i | \sigma)\)</span> and the total likelihood for the reparametrized family is <span class="math notranslate nohighlight">\(L^\prime(V) = \prod_i p^\prime(x_i | V)\)</span>. Let’s make two helper functions.</p>
<p>We will then plot the original and reparametrized likelihoods and see that the value of the likelihood is invariant to reprametrization: eg.  <span class="math notranslate nohighlight">\(L(\sigma) = L^\prime(V(\sigma))\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">sigma</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">*=</span> <span class="n">my_family_of_pdfs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">sigma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span>

<span class="k">def</span> <span class="nf">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">variance</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">*=</span> <span class="n">my_reparametrized_family_of_pdfs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">variance</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s compare <span class="math notranslate nohighlight">\(L(\sigma_{true})\)</span> and <span class="math notranslate nohighlight">\(L^\prime(V(\sigma_{true}))\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">true_sigma</span><span class="p">)),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">true_V</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(47.12737234655253, 47.12737234655253)
</pre></div>
</div>
</div>
</div>
<p>Let’s compare <span class="math notranslate nohighlight">\(L(\sigma_{other})\)</span> and <span class="math notranslate nohighlight">\(L^\prime(V(\sigma_{other}))\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">other_sigma</span><span class="p">)),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">other_V</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(57.1805758896008, 57.1805758896008)
</pre></div>
</div>
</div>
</div>
<p>They are equal!</p>
</div>
<div class="section" id="make-matched-pairs-for-the-parameters">
<h3>Make matched pairs for the parameters<a class="headerlink" href="#make-matched-pairs-for-the-parameters" title="Permalink to this headline">¶</a></h3>
<p>Note, we are <strong>not</strong> using <code class="docutils literal notranslate"><span class="pre">np.linspace</span></code> twice, we are making pairs such that  <code class="docutils literal notranslate"><span class="pre">V_for_plot[i]</span> <span class="pre">=</span> <span class="pre">sigma_for_plot[i]**2</span></code>. This is important for understanding the last three plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma_for_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">V_for_plot</span> <span class="o">=</span> <span class="n">sigma_to_V</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">)</span>
<span class="c1">#V_for_plot = np.linspace(sigma_to_V(.5),sigma_to_V(2.5)) ## NOT THIS, it would make a line in the next plot</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">,</span> <span class="n">V_for_plot</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\sigma$ vs. $V$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">,</span> <span class="n">sigma_for_plot</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\sigma$ vs. $\sigma^2$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">),</span> <span class="n">V_for_plot</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\sqrt</span><span class="si">{V}</span><span class="s1">$ vs. $V$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">,</span> <span class="n">true_V</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$(\sigma_</span><span class="si">{true}</span><span class="s1">, V_</span><span class="si">{true}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">other_sigma</span><span class="p">,</span> <span class="n">other_V</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$(\sigma_</span><span class="si">{other}</span><span class="s1">, V_</span><span class="si">{other}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$V = \sigma^2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_25_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_25_0.png" />
</div>
</div>
<p>Note that above these three curves  are identical.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>x-axis</p></th>
<th class="text-align:center head"><p>y-axis</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">V_for_plot</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot**2</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">np.sqrt(V_for_plot)</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">V_for_plot</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="plot-the-likelihoods">
<h3>Plot the likelihoods<a class="headerlink" href="#plot-the-likelihoods" title="Permalink to this headline">¶</a></h3>
<p>First, evaluate the original and reparametrized likelihoods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">original_likelihood</span> <span class="o">=</span> <span class="n">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sigma_for_plot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reparametrized_likelihood</span> <span class="o">=</span> <span class="n">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">V_for_plot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This is somewhat subtle. When we use <code class="docutils literal notranslate"><span class="pre">plt.plot(x,y)</span></code> it assumes that <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are arrays and that <code class="docutils literal notranslate"><span class="pre">x[i]</span></code> and <code class="docutils literal notranslate"><span class="pre">y[i]</span></code> are paired. We took care to make this be the case. So we can mix and match the input and output. If the x-axis corresponds to the input, we are directly plotting the likeilhood as it was evaluated. However, if we mix the x-axis and the input then we are reparametrizing the likelihood via plotting not via evaluation.</p>
<p>First consider this pair of plots. The first row is the direct evaluation of the original likelihood <span class="math notranslate nohighlight">\(L(\sigma)\)</span>. The second row is the direct evaluation of the reparametrized likelihood <span class="math notranslate nohighlight">\(L^\prime(V)\)</span>, but plotted against <span class="math notranslate nohighlight">\(\sqrt{V}\)</span>. The fact that they match shows that the value of the likelihood is invariant to reparameterization.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Input</p></th>
<th class="text-align:center head"><p>Output</p></th>
<th class="text-align:center head"><p>x-axis for plot</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">original_likelihood</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">V_for_plot</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">reparametrized_likelihood</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">np.sqrt(V_for_plot)</span></code></p></td>
</tr>
</tbody>
</table>
<p>Note the use of a secondary axis on the top of the plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">original_likelihood</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;original $-2 \log L(\sigma))$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">),</span> <span class="o">-</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">reparametrized_likelihood</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;reparameterized $-2 \log L^\prime(\sqrt</span><span class="si">{V}</span><span class="s1">)$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">true_sigma</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$-2 \log L(\sigma=\sigma_</span><span class="si">{true}</span><span class="s1">) $&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">other_sigma</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">other_sigma</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$-2 \log L(\sigma=\sigma_</span><span class="si">{other}</span><span class="s1">) $&#39;</span><span class="p">)</span>


<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$-2 \log L$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">secax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">secondary_xaxis</span><span class="p">(</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">functions</span><span class="o">=</span><span class="p">(</span><span class="n">sigma_to_V</span><span class="p">,</span> <span class="n">V_to_sigma</span><span class="p">))</span>
<span class="n">secax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$V$&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_31_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_31_0.png" />
</div>
</div>
<p>The second plot is similar. The first row is the direct evaluation of the reparametrized likelihood <span class="math notranslate nohighlight">\(L^\prime(V)\)</span>.
The second row is the direct evaluation of the original likelihood <span class="math notranslate nohighlight">\(L(\sigma)\)</span>, but plotted against <span class="math notranslate nohighlight">\(\sigma^2\)</span>. The fact that they match shows, again, that the value of the likelihood is invariant to reparameterization.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Input</p></th>
<th class="text-align:center head"><p>Output</p></th>
<th class="text-align:center head"><p>x-axis for plot</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">original_likelihood</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">sigma_for_plot**2</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">V_for_plot</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">reparametrized_likelihood</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">V_for_plot</span></code></p></td>
</tr>
</tbody>
</table>
<p>Note the use of a secondary axis on the top of the plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">original_likelihood</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;original $-2 \log L(\sigma^2))$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">reparametrized_likelihood</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;reparameterized $-2 \log L^\prime(V)$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">true_V</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">true_V</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$-2 \log L^\prime(V=V_</span><span class="si">{true}</span><span class="s1">) $&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">other_V</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">other_V</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$-2 \log L^\prime(V=V_</span><span class="si">{other}</span><span class="s1">) $&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$-2 \log L$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$V$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span><span class="mf">2.6</span><span class="p">)</span> <span class="c1">#need to avoid some error with secondary axis with lower lim = 0</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">secax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">secondary_xaxis</span><span class="p">(</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">functions</span><span class="o">=</span><span class="p">(</span><span class="n">V_to_sigma</span><span class="p">,</span><span class="n">sigma_to_V</span><span class="p">))</span>
<span class="n">secax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma $&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_33_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_33_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="equivariance-of-the-mle">
<h2>Equivariance of the MLE<a class="headerlink" href="#equivariance-of-the-mle" title="Permalink to this headline">¶</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <strong>maximum likelihood</strong> value of the parameters transform <strong>equivariantly</strong>.</p>
<p>Physicists often use the term <em>covariant</em> (or <em>contravariant</em>) in this context (eg. the way Lorentz vectors transform under a boost), but this term may be confused with the <span class="xref myst">covariance of two random variables</span>.</p>
</div>
<p>For example, if</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}_{MLE} := \textrm{argmax}_\sigma L(\sigma)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\hat{V}_{MLE} := \textrm{argmax}_V L^\prime(V)
\]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}_{MLE} = \sqrt{\hat{V}}_{MLE}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\hat{V}_{MLE} = \left ( \hat{\sigma}_{MLE} \right)^2
\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Often people say that the maximum likelhood estimator is <em>invariant</em> to reparameterization, but this is sloppy language. Invariant would mean <span class="math notranslate nohighlight">\(\hat{\sigma} = \hat{V}\)</span>. Intuitively the point is that it doesn’t matter which parameterization you use, you will get the same answer (and the transformation from <span class="math notranslate nohighlight">\(\sigma \to V\)</span> is implied).</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The equivariance condition can also be understood as the following commutative diagram representing <span class="math notranslate nohighlight">\(f(g·x) = g·f(x)\)</span>. Here <span class="math notranslate nohighlight">\(f\)</span> would represent transforming from <span class="math notranslate nohighlight">\(\sigma \to V\)</span> (eg. <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> in the figure) and <span class="math notranslate nohighlight">\(g\)</span> would represent finding the maximum likelihood for the parameter in question.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Equivariant_commutative_diagram.png/1200px-Equivariant_commutative_diagram.png" alt="Equivariant commutative diagram.png" width="30%"><br><a href="http://creativecommons.org/licenses/by-sa/3.0/" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=34670">Link</a></p>
</div>
<div class="section" id="id1">
<h3>Check explicitly<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Let’s check this explicitly using <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">likelihood_wrapper</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">other_sigma</span><span class="p">])</span>
<span class="n">result_from_orig</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">likelihood_wrapper</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;nelder-mead&#39;</span><span class="p">,</span>  <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xatol&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="n">sigma_MLE</span> <span class="o">=</span> <span class="n">result_from_orig</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 46.697141
         Iterations: 28
         Function evaluations: 56
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reparametrized_likelihood_wrapper</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">my_reparametrized_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">other_V</span><span class="p">])</span>
<span class="n">result_from_rep</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">reparametrized_likelihood_wrapper</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;nelder-mead&#39;</span><span class="p">,</span>  <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xatol&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="n">V_MLE</span> <span class="o">=</span> <span class="n">result_from_rep</span><span class="o">.</span><span class="n">x</span>
<span class="n">likelihood_at_V_MLE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">result_from_rep</span><span class="o">.</span><span class="n">fun</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 46.697141
         Iterations: 31
         Function evaluations: 65
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma_MLE</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V_MLE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([1.14755414]), array([1.14755414]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma_MLE</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">V_MLE</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([1.31688049]), array([1.31688051]))
</pre></div>
</div>
</div>
</div>
<p>Fantastic!</p>
</div>
</div>
<div class="section" id="comparison-to-maximum-a-posteriori-map-estimate">
<h2>Comparison to maximum a posteriori (MAP) estimate<a class="headerlink" href="#comparison-to-maximum-a-posteriori-map-estimate" title="Permalink to this headline">¶</a></h2>
<p>In a Bayesian analysis one would promote <span class="math notranslate nohighlight">\(\sigma\)</span> to a random variable <span class="math notranslate nohighlight">\(\Sigma\)</span> and also have a prior <span class="math notranslate nohighlight">\(p_\Sigma(\Sigma)\)</span>.</p>
<p>From this one would calculate the posterior using Bayes’ theorem</p>
<div class="math notranslate nohighlight">
\[
p_{\Sigma|X}(\Sigma | X=x) = \frac{p_{X|\Sigma}(X=x \mid \Sigma) p_\Sigma(\Sigma)}{p_X(X=x)}
\]</div>
<p>or more colloquially</p>
<div class="math notranslate nohighlight">
\[
p(\sigma | x) = \frac{p(x \mid \sigma) p(\sigma)}{p(x)}
\]</div>
<p>From this Bayesian analysis often considers the <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Limitations"><strong>maximum a posteriori</strong></a> point estimate, or MAP.</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}_{MAP} := \textrm{argmax}_\sigma p(\sigma|x)
\]</div>
<p>Note that if we change variables from <span class="math notranslate nohighlight">\(\sigma \to V\)</span>, then the posterior will transform in a way that includes the Jacobian factor <span class="math notranslate nohighlight">\(|\partial V / \partial \sigma| = 2 \sigma = 2 \sqrt{V}\)</span>. This same Jacobian factor also applies to the prior <span class="math notranslate nohighlight">\(p(\theta) \to p(V)\)</span>, which makes sense given that the likelihood and the normalizing constant are invariant under reparametrization. One can ask, what is the MAP estimate for <span class="math notranslate nohighlight">\(V\)</span>?</p>
<div class="math notranslate nohighlight">
\[
\hat{V}_{MAP} := \textrm{argmax}_V p(V|x)
\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Unlike maximum likelihood estimators, the MAP estimate is not equivariant (covariant) to reparameterization.</p>
<p>Switching from one parameterization to another involves introducing a Jacobian that impacts on the location of the maximum.</p>
</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[
\hat{V}_{MAP}  \ne \hat{\sigma}_{MAP}^2
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}_{MAP}  \ne \sqrt{\hat{V}_{MAP} }
\]</div>
</div>
<div class="section" id="the-myth-of-the-uninformative-uniform-prior">
<h2>The myth of the uninformative uniform prior<a class="headerlink" href="#the-myth-of-the-uninformative-uniform-prior" title="Permalink to this headline">¶</a></h2>
<p>A common logical fallacy is <a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_of_the_inverse">Confusion of the inverse</a>. Here is a snippet from Wikipedia</p>
<blockquote>
<div><p>Confusion of the inverse, also called the conditional probability fallacy or the inverse fallacy, is a logical fallacy whereupon a conditional probability is equated with its inverse; that is, given two events A and B, the probability of A happening given that B has happened is assumed to be about the same as the probability of B given A, when there is actually no evidence for this assumption. More formally, P(A|B) is assumed to be approximately equal to P(B|A).</p>
</div></blockquote>
<p>This inverse fallacy is closely connected to the common tendency to think of the posterior and likelihood as being proportional <span class="math notranslate nohighlight">\(p(\theta|x) \propto p(x|\theta)\)</span>, which is the case if one were to place a uniform prior on <span class="math notranslate nohighlight">\(\theta\)</span> (or implicitly setting <span class="math notranslate nohighlight">\(p(\theta)=1\)</span> and not worrying about the normalization of the prior as long as the posterior is normalized appropriately).</p>
<p>Putting a uniform prior on a parameter <span class="math notranslate nohighlight">\(\theta\)</span> is deceptively attractive. Often people say that this is an uninformative prior because it assigns equal probability (density) to all values of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>A uniform prior in <span class="math notranslate nohighlight">\(\theta\)</span> is <strong>not</strong> uniform in a different parameterization <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
</div>
<p>As we saw in <span class="xref myst"><em>How do distributions transform under a change of variables?</em></span>, the uniform distribution for the angle <span class="math notranslate nohighlight">\(x \in [0, 2\pi]\)</span> was dramatically transformed when we changed variables to <span class="math notranslate nohighlight">\(\cos(x)\)</span>. Alternatively, any informative prior is uniform under the appropriate reparameterization.</p>
<p>So if we:</p>
<ul class="simple">
<li><p>place a uniform prior on <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>compute the posterior <span class="math notranslate nohighlight">\(p(\theta|x) \propto p(x|\theta)\)</span></p></li>
<li><p>change variables from <span class="math notranslate nohighlight">\(\theta \to \phi\)</span>, which results in <span class="math notranslate nohighlight">\(p(\phi|x) = p(\phi|x) / |\partial \phi / \partial \theta | \propto p(x|\phi) / |\partial \phi / \partial \theta\)</span></p></li>
</ul>
<p>We will not get the same results as if we:</p>
<ul class="simple">
<li><p>reparametrize from <span class="math notranslate nohighlight">\(\theta \to \phi\)</span></p></li>
<li><p>place a uniform prior on <span class="math notranslate nohighlight">\(\phi\)</span></p></li>
<li><p>compute the posterior <span class="math notranslate nohighlight">\(p^\prime(\phi | x) \propto p(x | \phi)\)</span></p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This can also be thought of as <strong>a violation of equivariance</strong> in the following commutative diagram representing <span class="math notranslate nohighlight">\(f(g·x) = g·f(x)\)</span>. Here <span class="math notranslate nohighlight">\(f\)</span> would represent would represent transforming from <span class="math notranslate nohighlight">\(\theta \to \phi\)</span> (eg. <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> in the diagram), and <span class="math notranslate nohighlight">\(g\)</span> would be “derive a posterior by placing a uniform prior on the parameter in question”.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Equivariant_commutative_diagram.png/1200px-Equivariant_commutative_diagram.png" alt="Equivariant commutative diagram.png" width="30%"><br><a href="http://creativecommons.org/licenses/by-sa/3.0/" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=34670">Link</a></p>
</div>
<p>This is would not be the case if <span class="math notranslate nohighlight">\(\theta\)</span> were discrete, and the change of variables from <span class="math notranslate nohighlight">\(\theta\)</span> to <span class="math notranslate nohighlight">\(\phi\)</span> was 1-to-1. In that case, the argument for a uniform prior being uninformative makes more sense.</p>
</div>
<div class="section" id="demonstration-of-likelihood-vs-posterior-under-reparameterization">
<h2>Demonstration of likelihood vs. posterior under reparameterization<a class="headerlink" href="#demonstration-of-likelihood-vs-posterior-under-reparameterization" title="Permalink to this headline">¶</a></h2>
<p>Let’s assume a uniform prior for <span class="math notranslate nohighlight">\(\sigma \in [0.5, 2.5]\)</span>, so <span class="math notranslate nohighlight">\(p(\sigma) = \frac{1}{2}\)</span>.</p>
<p>This will imply a prior for <span class="math notranslate nohighlight">\(V \in [0.5^2, 2.5^2]\)</span> that is <span class="math notranslate nohighlight">\(p(V) = \frac{1}{2}\frac{1}{2 \sqrt{V}}\)</span></p>
<p>Let’s plot the likelihood and posterior vs. <span class="math notranslate nohighlight">\(\sigma\)</span> and <span class="math notranslate nohighlight">\(V\)</span>.</p>
<div class="section" id="sampling-the-posterior">
<h3>Sampling the posterior<a class="headerlink" href="#sampling-the-posterior" title="Permalink to this headline">¶</a></h3>
<p>It’s easy to make mistakes when predicting the transformed distribution, so let’s check by drawing samples <span class="math notranslate nohighlight">\(\{\sigma_i\}\)</span> from the posterior, transforming them directly with <span class="math notranslate nohighlight">\(V_i = \sigma_i^2\)</span>, and then histograming them.</p>
<p>To do this we will use <span class="xref myst">the accept / reject algorithm</span> (aka <a class="reference external" href="prml_notebooks/ch11_Sampling_Methods.html#rejection-sampling">rejection sampling</a> ).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_accep_rej</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s_accep_rej</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">N_accep_rej</span><span class="p">)</span>
<span class="n">y_accep_rej</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">original_likelihood</span><span class="p">),</span> <span class="n">N_accep_rej</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">y_accep_rej</span><span class="o">&lt;</span><span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">s_accep_rej</span><span class="p">,</span> <span class="n">sigma_for_plot</span><span class="p">,</span> <span class="n">original_likelihood</span><span class="p">)</span>
<span class="n">sigma_posterior_samples</span> <span class="o">=</span> <span class="n">s_accep_rej</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="normalizing-the-posterior">
<h3>Normalizing the posterior<a class="headerlink" href="#normalizing-the-posterior" title="Permalink to this headline">¶</a></h3>
<p>With a uniform prior in <span class="math notranslate nohighlight">\(\sigma\)</span>, the posterior is proportional to the likelihood, but we still need to normalize (the denominator in Bayes’ theorem). We will use <code class="docutils literal notranslate"><span class="pre">scipy.integrate</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.integrate</span> <span class="k">as</span> <span class="nn">integrate</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">posterior_sigma_norm</span><span class="p">,</span> <span class="n">uncertainty</span><span class="p">)</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">sigma_for_plot</span><span class="p">,</span> <span class="n">original_likelihood</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">posterior_sigma_norm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.048436794345585e-11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">posterior_in_sigma</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma_for_plot</span><span class="p">,</span> <span class="n">original_likelihood</span><span class="p">)</span><span class="o">/</span><span class="n">posterior_sigma_norm</span>

<span class="k">def</span> <span class="nf">prior_in_sigma</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mf">2.5</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sigma</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="make-the-plot-for-sigma">
<h3>Make the plot for <span class="math notranslate nohighlight">\(\sigma\)</span><a class="headerlink" href="#make-the-plot-for-sigma" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sigma_posterior_samples</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;posterior samples&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">,</span> <span class="n">posterior_in_sigma</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;normalized posterior $p(\sigma | x)$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">),</span> <span class="p">(</span><span class="n">original_likelihood</span><span class="p">)</span><span class="o">/</span><span class="n">posterior_sigma_norm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;rescaled likelihood $L(\sigma))/Z$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">,</span> <span class="n">prior_in_sigma</span><span class="p">(</span><span class="n">sigma_for_plot</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;uniform prior $p(\sigma)$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sigma_MLE</span><span class="p">,</span> <span class="n">posterior_in_sigma</span><span class="p">(</span><span class="n">sigma_MLE</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MLE / MAP in $\sigma$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;posterior&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">secax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">secondary_xaxis</span><span class="p">(</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">functions</span><span class="o">=</span><span class="p">(</span><span class="n">sigma_to_V</span><span class="p">,</span> <span class="n">V_to_sigma</span><span class="p">))</span>
<span class="n">secax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$V$&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_50_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_50_0.png" />
</div>
</div>
<p>Notice the following:</p>
<ul class="simple">
<li><p>The prior <strong>is</strong>  uniform in <span class="math notranslate nohighlight">\(\sigma\)</span> (is it uninformative?)</p></li>
<li><p>The posterior and the likelihood have the same shape – they are proportional to each other.</p>
<ul>
<li><p>the MLE and MAP coincide</p></li>
</ul>
</li>
<li><p>Our accept/reject samples agree with the posterior distribution</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma_MAP</span> <span class="o">=</span> <span class="n">sigma_MLE</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="make-the-same-plot-for-v">
<h3>Make the same plot for <span class="math notranslate nohighlight">\(V\)</span><a class="headerlink" href="#make-the-same-plot-for-v" title="Permalink to this headline">¶</a></h3>
<p>First we transforms the samples from the prior deterministically. This is the most robust and least error prone way to check. It’s also most closely tied the definition of what it means to change variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V_posterior_samples</span> <span class="o">=</span> <span class="n">sigma_to_V</span><span class="p">(</span><span class="n">sigma_posterior_samples</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we will use the result from our change of variables formula to predict the posterior and prior in <span class="math notranslate nohighlight">\(V\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">posterior_in_V</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="n">sigma_</span> <span class="o">=</span> <span class="n">V_to_sigma</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
    <span class="n">jacobian</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">sigma_</span> <span class="c1">#doing it by hand</span>
    <span class="k">return</span> <span class="n">posterior_in_sigma</span><span class="p">(</span><span class="n">sigma_</span><span class="p">)</span><span class="o">/</span><span class="n">jacobian</span>

<span class="k">def</span> <span class="nf">prior_in_V</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="n">sigma_</span> <span class="o">=</span> <span class="n">V_to_sigma</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
    <span class="n">jacobian</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">sigma_</span> <span class="c1">#doing it by hand</span>
    <span class="n">uniform_prior_in_sigma</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mf">2.5</span><span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">uniform_prior_in_sigma</span><span class="o">/</span><span class="n">jacobian</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check that both the prior and posterior are normalized properly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#check normalization for posterior in V, should be 1</span>
<span class="p">(</span><span class="n">V_posterior_norm</span><span class="p">,</span> <span class="n">V_uncertainty</span><span class="p">)</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">V</span><span class="p">:</span> <span class="n">posterior_in_V</span><span class="p">(</span><span class="n">V</span><span class="p">),</span> <span class="mf">0.5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;norm of posterior is </span><span class="si">{0}</span><span class="s1"> +/- </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V_posterior_norm</span><span class="p">,</span> <span class="n">V_uncertainty</span><span class="p">))</span>
<span class="c1">#don&#39;t worry about warning here, integration is good enough to make the point</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>norm of posterior is 0.9997008924989026 +/- 7.28054934385769e-05
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-58-0d96b2dc5a03&gt;:2: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.
  If increasing the limit yields no improvement it is advised to analyze 
  the integrand in order to determine the difficulties.  If the position of a 
  local difficulty can be determined (singularity, discontinuity) one will 
  probably gain from splitting up the interval and calling the integrator 
  on the subranges.  Perhaps a special-purpose integrator should be used.
  (V_posterior_norm, V_uncertainty) = integrate.quad(lambda V: posterior_in_V(V), 0.5**2, 2.5**2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#check normalization for prior in V, should be 1</span>
<span class="p">(</span><span class="n">V_prior_norm</span><span class="p">,</span> <span class="n">V_uncertainty</span><span class="p">)</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">V</span><span class="p">:</span> <span class="n">prior_in_V</span><span class="p">(</span><span class="n">V</span><span class="p">),</span> <span class="mf">0.5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">V_prior_norm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0000000000000073
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#calculate integral of likelihood in V, just for plotting purposes</span>
<span class="p">(</span><span class="n">V_L_norm</span><span class="p">,</span> <span class="n">uncertainty</span><span class="p">)</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">V</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">V_for_plot</span><span class="p">,</span> <span class="n">reparametrized_likelihood</span><span class="p">),</span> <span class="mf">0.5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">V_L_norm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0155480361220947e-10
</pre></div>
</div>
</div>
</div>
<p>let’s find the MAP</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">other_V</span><span class="p">])</span>
<span class="n">result_from_rep</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">V</span><span class="p">:</span> <span class="o">-</span><span class="n">posterior_in_V</span><span class="p">(</span><span class="n">V</span><span class="p">),</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;nelder-mead&#39;</span><span class="p">,</span>  <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xatol&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="n">V_MAP</span> <span class="o">=</span> <span class="n">result_from_rep</span><span class="o">.</span><span class="n">x</span>
<span class="n">posterior_at_V_MAP</span> <span class="o">=</span> <span class="o">-</span><span class="n">result_from_rep</span><span class="o">.</span><span class="n">fun</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: -0.792190
         Iterations: 31
         Function evaluations: 62
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">V_posterior_samples</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;posterior samples&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">,</span> <span class="n">posterior_in_V</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;normalized posterior $p(V| x)$&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">,</span> <span class="p">(</span><span class="n">reparametrized_likelihood</span><span class="p">)</span><span class="o">/</span><span class="n">V_L_norm</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;rescaled likelihood $L^\prime(V)$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">,</span> <span class="n">prior_in_V</span><span class="p">(</span><span class="n">V_for_plot</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;transformed prior $p(V)$$&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">V_MLE</span><span class="p">,</span> <span class="n">likelihood_at_V_MLE</span><span class="o">/</span><span class="n">V_L_norm</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\hat</span><span class="si">{V}</span><span class="s1">_</span><span class="si">{MLE}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">V_MAP</span><span class="p">,</span> <span class="n">posterior_at_V_MAP</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\hat</span><span class="si">{V}</span><span class="s1">_</span><span class="si">{MAP}</span><span class="s1">$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$posterior$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$V$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mf">2.5</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#need to avoid some error with secondary axis with lower lim = 0</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">secax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">secondary_xaxis</span><span class="p">(</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">functions</span><span class="o">=</span><span class="p">(</span><span class="n">V_to_sigma</span><span class="p">,</span><span class="n">sigma_to_V</span><span class="p">))</span>
<span class="n">secax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma $&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/invariance-of-likelihood-to-reparameterizaton_63_0.png" src="../_images/invariance-of-likelihood-to-reparameterizaton_63_0.png" />
</div>
</div>
<p>Check the MLE and the MAP for <span class="math notranslate nohighlight">\(V\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V_MLE</span><span class="p">,</span> <span class="n">V_MAP</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([1.31688051]), array([1.23708871]))
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="violation-of-equivaraince-for-the-map-and-posterior">
<h3>Violation of equivaraince for the MAP and posterior<a class="headerlink" href="#violation-of-equivaraince-for-the-map-and-posterior" title="Permalink to this headline">¶</a></h3>
<p>Notice the following:</p>
<ul class="simple">
<li><p>Remember, we know theand the MLE is equivariant and <span class="math notranslate nohighlight">\(\hat{\sigma}_{MLE}=\hat{\sigma}_{MAP}\)</span>, but</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\hat{V}_{MLE} \ne \hat{V}_{MAP}\)</span>. Therefore the MAP is not equivariant</p></li>
</ul>
</li>
<li><p>Remember, we know the likelihood is invariant, but the posterior and the likelihood are <strong>no longer</strong> proportional to each other anymore.</p>
<ul>
<li><p>Therefore we do not obtain the same posterior for <span class="math notranslate nohighlight">\(V\)</span> if we apply a uniform prior in <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
<li><p>The uniform prior in <span class="math notranslate nohighlight">\(\sigma\)</span> <strong>does not</strong> correspond to a uniform prior <span class="math notranslate nohighlight">\(V\)</span>, thus it is not uninformative for <span class="math notranslate nohighlight">\(V\)</span> even though this is just the resut of a reparameterization of the same likelihood, prior, and posterior. The converse also holds, a uniform prior in <span class="math notranslate nohighlight">\(V\)</span> is not uninformative for <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Think carefully before you put a uniform prior on whatever parameters are presented to you, and do not fool yourself into thinking a uniform prior is uninformative in some universal, parameterization-independent way.</p>
</div>
</div>
</div>
<div class="section" id="food-for-thought">
<h2>Food for thought<a class="headerlink" href="#food-for-thought" title="Permalink to this headline">¶</a></h2>
<div class="tip admonition">
<p class="admonition-title">Food for thought: Equipartion of energy</p>
<p>A key concept in statistical mechanics is the <strong>equipartion of energy</strong>, which assigns equal probability to states with the same energy. This is uniform distribution. If we are working with discrete states, then there is no ambiguity, but how do we understand equipartition of energy for continuous state spaces? In what variables is the system uniformly distributed?</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Food for thought: Lorentz Invariant Phase space</p>
<p>In particle physics, the differential scattering cross-section is often factorized into the product of a squared-matrix element and a phase space term. This is like Fermi’s golden rule that is a product of a transition rate( <span class="math notranslate nohighlight">\(\propto\)</span> probability per unit time) and a density of states. It is also structurally similar to a product of a likelhood and a prior, where phase space plays the role of the prior. In what variables is the Lorentz invariant phase space uniform (and why)?</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./distributions"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="likelihood-change-obs.html" title="previous page">Transformation of likelihood with change of random variable</a>
    <a class='right-next' id="next-link" href="../error-propagation/investigating-propagation-of-errors.html" title="next page">Investigating propagation of errors</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kyle Cranmer<br/>
        
            &copy; Copyright .<br/>
          <div class="extra_footer">
            <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
    All content on this site (unless otherwise specified) is licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0 license</a>
</div>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-178330963-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>